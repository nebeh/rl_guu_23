{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0c27221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action space: Discrete(2)\n",
      "Observation space: Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "print(f\"Action space: {env.action_space}\")\n",
    "print(f\"Observation space: {env.observation_space}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "342855d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import count\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f183526",
   "metadata": {},
   "source": [
    "Модель Actor-Critic является дальнейшим развитием градиентов политики, в которой мы строим нейронную сеть для обучения как политике, так и оценкам вознаграждения. Сеть будет иметь два выхода (или вы можете рассматривать ее как две отдельные сети):\n",
    "\n",
    "- Actor будет рекомендовать действия, предоставляя нам распределение вероятности состояния, как в градиентной модели политики.\n",
    "- Critic оценивает, какова будет награда от этих действий. Он возвращает суммарное предполагаемое вознаграждение в будущем при данном состоянии.\n",
    "Определим такую модель:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35a6c7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "lr = 0.0001\n",
    "\n",
    "class Actor(torch.nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(Actor, self).__init__()\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.linear1 = torch.nn.Linear(self.state_size, 128)\n",
    "        self.linear2 = torch.nn.Linear(128, 256)\n",
    "        self.linear3 = torch.nn.Linear(256, self.action_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        output = F.relu(self.linear1(state))\n",
    "        output = F.relu(self.linear2(output))\n",
    "        output = self.linear3(output)\n",
    "        distribution = torch.distributions.Categorical(F.softmax(output, dim=-1))\n",
    "        return distribution\n",
    "\n",
    "\n",
    "class Critic(torch.nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(Critic, self).__init__()\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.linear1 = torch.nn.Linear(self.state_size, 128)\n",
    "        self.linear2 = torch.nn.Linear(128, 256)\n",
    "        self.linear3 = torch.nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, state):\n",
    "        output = F.relu(self.linear1(state))\n",
    "        output = F.relu(self.linear2(output))\n",
    "        value = self.linear3(output)\n",
    "        return value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1735e5b6",
   "metadata": {},
   "source": [
    "Для этого нам потребуется немного изменить функции discounted_rewards и run_episode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "941562e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discounted_rewards(next_value, rewards, masks, gamma=0.99):\n",
    "    R = next_value\n",
    "    returns = []\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        R = rewards[step] + gamma * R * masks[step]\n",
    "        returns.insert(0, R)\n",
    "    return returns\n",
    "\n",
    "def run_episode(actor, critic, n_iters):\n",
    "    optimizerA = torch.optim.Adam(actor.parameters())\n",
    "    optimizerC = torch.optim.Adam(critic.parameters())\n",
    "    for iter in range(n_iters):\n",
    "        state = env.reset()\n",
    "        state = state[0]\n",
    "        log_probs = []\n",
    "        values = []\n",
    "        rewards = []\n",
    "        masks = []\n",
    "        entropy = 0\n",
    "        env.reset()\n",
    "\n",
    "        for i in count():\n",
    "            env.render()\n",
    "            state = torch.FloatTensor(state).to(device)\n",
    "            dist, value = actor(state), critic(state)\n",
    "\n",
    "            action = dist.sample()\n",
    "            next_state, reward, done, _,_ = env.step(action.cpu().numpy())\n",
    "\n",
    "            log_prob = dist.log_prob(action).unsqueeze(0)\n",
    "            entropy += dist.entropy().mean()\n",
    "\n",
    "            log_probs.append(log_prob)\n",
    "            values.append(value)\n",
    "            rewards.append(torch.tensor([reward], dtype=torch.float, device=device))\n",
    "            masks.append(torch.tensor([1-done], dtype=torch.float, device=device))\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "            if done:\n",
    "                print('Iteration: {}, Score: {}'.format(iter, i))\n",
    "                break\n",
    "\n",
    "\n",
    "        next_state = torch.FloatTensor(next_state).to(device)\n",
    "        next_value = critic(next_state)\n",
    "        returns = discounted_rewards(next_value, rewards, masks)\n",
    "\n",
    "        log_probs = torch.cat(log_probs)\n",
    "        returns = torch.cat(returns).detach()\n",
    "        values = torch.cat(values)\n",
    "\n",
    "        advantage = returns - values\n",
    "\n",
    "        actor_loss = -(log_probs * advantage.detach()).mean()\n",
    "        critic_loss = advantage.pow(2).mean()\n",
    "\n",
    "        optimizerA.zero_grad()\n",
    "        optimizerC.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        critic_loss.backward()\n",
    "        optimizerA.step()\n",
    "        optimizerC.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5690da12",
   "metadata": {},
   "source": [
    "Теперь запустим основной цикл обучения. Мы будем использовать ручной процесс обучения сети, вычисляя соответствующие функции потерь и обновляя параметры сети:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d3f8e5cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0, Score: 21\n",
      "Iteration: 1, Score: 19\n",
      "Iteration: 2, Score: 60\n",
      "Iteration: 3, Score: 9\n",
      "Iteration: 4, Score: 11\n",
      "Iteration: 5, Score: 22\n",
      "Iteration: 6, Score: 22\n",
      "Iteration: 7, Score: 18\n",
      "Iteration: 8, Score: 10\n",
      "Iteration: 9, Score: 11\n",
      "Iteration: 10, Score: 18\n",
      "Iteration: 11, Score: 15\n",
      "Iteration: 12, Score: 13\n",
      "Iteration: 13, Score: 12\n",
      "Iteration: 14, Score: 22\n",
      "Iteration: 15, Score: 17\n",
      "Iteration: 16, Score: 14\n",
      "Iteration: 17, Score: 8\n",
      "Iteration: 18, Score: 21\n",
      "Iteration: 19, Score: 12\n",
      "Iteration: 20, Score: 11\n",
      "Iteration: 21, Score: 21\n",
      "Iteration: 22, Score: 13\n",
      "Iteration: 23, Score: 33\n",
      "Iteration: 24, Score: 13\n",
      "Iteration: 25, Score: 35\n",
      "Iteration: 26, Score: 12\n",
      "Iteration: 27, Score: 16\n",
      "Iteration: 28, Score: 14\n",
      "Iteration: 29, Score: 30\n",
      "Iteration: 30, Score: 10\n",
      "Iteration: 31, Score: 20\n",
      "Iteration: 32, Score: 30\n",
      "Iteration: 33, Score: 46\n",
      "Iteration: 34, Score: 32\n",
      "Iteration: 35, Score: 29\n",
      "Iteration: 36, Score: 30\n",
      "Iteration: 37, Score: 21\n",
      "Iteration: 38, Score: 16\n",
      "Iteration: 39, Score: 30\n",
      "Iteration: 40, Score: 13\n",
      "Iteration: 41, Score: 16\n",
      "Iteration: 42, Score: 16\n",
      "Iteration: 43, Score: 30\n",
      "Iteration: 44, Score: 15\n",
      "Iteration: 45, Score: 37\n",
      "Iteration: 46, Score: 8\n",
      "Iteration: 47, Score: 10\n",
      "Iteration: 48, Score: 14\n",
      "Iteration: 49, Score: 23\n",
      "Iteration: 50, Score: 32\n",
      "Iteration: 51, Score: 44\n",
      "Iteration: 52, Score: 20\n",
      "Iteration: 53, Score: 51\n",
      "Iteration: 54, Score: 36\n",
      "Iteration: 55, Score: 13\n",
      "Iteration: 56, Score: 24\n",
      "Iteration: 57, Score: 23\n",
      "Iteration: 58, Score: 25\n",
      "Iteration: 59, Score: 26\n",
      "Iteration: 60, Score: 47\n",
      "Iteration: 61, Score: 42\n",
      "Iteration: 62, Score: 35\n",
      "Iteration: 63, Score: 27\n",
      "Iteration: 64, Score: 60\n",
      "Iteration: 65, Score: 43\n",
      "Iteration: 66, Score: 40\n",
      "Iteration: 67, Score: 25\n",
      "Iteration: 68, Score: 50\n",
      "Iteration: 69, Score: 30\n",
      "Iteration: 70, Score: 70\n",
      "Iteration: 71, Score: 29\n",
      "Iteration: 72, Score: 86\n",
      "Iteration: 73, Score: 37\n",
      "Iteration: 74, Score: 61\n",
      "Iteration: 75, Score: 109\n",
      "Iteration: 76, Score: 96\n",
      "Iteration: 77, Score: 29\n",
      "Iteration: 78, Score: 41\n",
      "Iteration: 79, Score: 32\n",
      "Iteration: 80, Score: 55\n",
      "Iteration: 81, Score: 30\n",
      "Iteration: 82, Score: 29\n",
      "Iteration: 83, Score: 25\n",
      "Iteration: 84, Score: 44\n",
      "Iteration: 85, Score: 56\n",
      "Iteration: 86, Score: 54\n",
      "Iteration: 87, Score: 63\n",
      "Iteration: 88, Score: 71\n",
      "Iteration: 89, Score: 40\n",
      "Iteration: 90, Score: 120\n",
      "Iteration: 91, Score: 31\n",
      "Iteration: 92, Score: 43\n",
      "Iteration: 93, Score: 59\n",
      "Iteration: 94, Score: 61\n",
      "Iteration: 95, Score: 52\n",
      "Iteration: 96, Score: 124\n",
      "Iteration: 97, Score: 61\n",
      "Iteration: 98, Score: 45\n",
      "Iteration: 99, Score: 58\n"
     ]
    }
   ],
   "source": [
    "actor = Actor(state_size, action_size).to(device)\n",
    "critic = Critic(state_size, action_size).to(device)\n",
    "run_episode(actor, critic, n_iters=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2799f7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
